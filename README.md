# Backpropagation Tutorial

This repository contains a step-by-step tutorial on implementing the backpropagation algorithm **from scratch** using NumPy — with no deep learning frameworks involved.

The goal is to help students and practitioners deeply understand how feedforward neural networks learn through gradient descent and loss minimization. The tutorial builds up from the mathematical intuition to a working implementation that learns classic tasks like **XOR**, and the **scikit-learn moons dataset**.

## Contents

- ✅ Feedforward neural network from scratch  
- ✅ Backpropagation implementation  
- ✅ Gradient descent for training  
- ✅ Task: XOR  
- ✅ Bonus: Moon classification
- ✅ No black boxes — everything is fully explained

## How to Use

Just open the notebook (`.ipynb` or `.py`) in Jupyter or Google Colab and follow along. Each section contains both explanations and runnable code blocks.

## Citation

If you found this notebook useful and want to reference it in your work, you can cite it as:

> *Leandro Risso Venâncio. (2025). Backpropagation from scratch — a step-by-step tutorial.*  
> [[GitHub Repo Link](https://github.com/lrvnc/backprop-tutorial/)]

Or simply link to the GitHub repository or video.
